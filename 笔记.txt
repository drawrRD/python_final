1.4 编码映射LabelEncoding  
看到有些特征为非数字类型，为了更好让模型理解和捕捉特征之间的相关性，

《异常值处理
MinMaxScaler  
大小相差较大, 为了去除特征尺度带来的影响
减小了异常值对模型、算法性能的影响

对原始数据进行变换把数据映射到 [0, 1] 之间

1.5数据分箱（变量离散化）

转化为有序的离散特征，便于后续的特征工程和建模分析。
保留了原始变量的一部分信息，并降低噪音，提供了鲁棒性,降低模型运算复杂度，提升模型运算速度
1.	可以减少离群值和噪声对整体的影响，从而增强模型的鲁棒性。
2.	通过对变量的离散化，帮助捕捉特征和目标之间的非线性关系，能够提高模型的表达能力。
3.	降低数据的维度，从而达到降低模型复杂度和过拟合风险的目的。
4.	离散后的特征利于模型的迭代，计算结果方便，
5.	特征离散后，模型会更加稳定。
》


1.6 特征筛选
计算特征之间的相关性矩阵，热力图可视化, 减小特征维度，
设置阈值0.01  防止过拟合噪声，提高计算效率 提高模型的稳定新
 
2.1 划分验证集
将subscribe一列删去后

2.2 模型选择
K折交叉验证

2.3 参数调整
网格搜索
GridSearchCV 指定的参数范围内找到精度最高的参数，但是这也是网格搜索的缺陷所在，他要求遍历所有可能参数的组合，在面对大数据集和多参数的情况下，非常耗时。

自动调参，只要把参数输进去，就能给出最优化结果和参数。但是这个方法适合于小数据集，一旦数据的量级上去了，很难得到结果。


2.4 模型训练
将交叉验证时得分最好的三个模型进行对比。

2.5 模型评价·
选择XGB，XGB的accuracy和f1值均为最高，说明它在捕捉真实正例和减少误报之间找到了一个较好的平衡，并且在识别正例和排除负例方面具有较高的能力。

在数据集中正负样本不平衡，no>>yes的情况下，
XGB模型通过加权损失函数，给予少数类别样本更高的权重，使得模型在预测时对少数样本有更高的召回率和准确度，也使得整体的预测效果最好。

这使得 XGBoost 模型在解决分类问题时更为优秀。
我们再将其未调参结果进行对比，明显发现调参后的模型预测更优


